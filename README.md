# ECE1512_2023F_ProjectRepo_BoWen

## Project A: Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks

This project aims to provide practical experience in model compression through knowledge distillation, enabling the transfer of knowledge from larger models to smaller, more practical models for real-world applications. It encompasses two main tasks:

Task 1: Implementing a conventional knowledge distillation framework to compress a model for the MNIST digit classification dataset. \


Task 2: Applying transfer learning and knowledge distillation to train a lightweight model that can mimic a pre-trained larger model on the MHIST clinical histopathology dataset.
